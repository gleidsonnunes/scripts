{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gleidsonnunes/scripts/blob/master/Narrador_XTTS_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ee81a3e",
      "metadata": {
        "id": "7ee81a3e"
      },
      "outputs": [],
      "source": [
        "!pip install TTS==0.21.1 gradio pymupdf ebooklib pydub bs4\n",
        "!apt-get update && apt-get install -y ffmpeg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3e9fac",
      "metadata": {
        "id": "1b3e9fac"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Espera-se que o arquivo enviado seja renomeado como 'voz_clonada_sample.wav'\n",
        "import shutil\n",
        "for name in uploaded:\n",
        "    if name.endswith(\".mp3\") or name.endswith(\".wav\"):\n",
        "        shutil.move(name, \"voz_clonada_sample.wav\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d10da9c2",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "d10da9c2",
        "outputId": "54aeb5b9-6772-4fcc-9e5c-fed8e4bb69d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
            " > Using model: xtts\n"
          ]
        },
        {
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL TTS.config.shared_configs.BaseDatasetConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([BaseDatasetConfig])` or the `torch.serialization.safe_globals([BaseDatasetConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-4225242036.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tts_models/multilingual/multi-dataset/xtts_v2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"neutral\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"happy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sad\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"angry\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"excited\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sleepy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"whispering\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shouting\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/TTS/api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, model_path, config_path, vocoder_path, vocoder_config_path, progress_bar, cs_api_model, gpu)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"tts_models\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"coqui_studio\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tts_model_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"voice_conversion_models\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_vc_model_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/TTS/api.py\u001b[0m in \u001b[0;36mload_tts_model_by_name\u001b[0;34m(self, model_name, gpu)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;31m# init synthesizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;31m# None values are fetch from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             self.synthesizer = Synthesizer(\n\u001b[0m\u001b[1;32m    186\u001b[0m                 \u001b[0mtts_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mtts_config_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/TTS/utils/synthesizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tts_checkpoint, tts_config_path, tts_speakers_file, tts_languages_file, vocoder_checkpoint, vocoder_config, encoder_checkpoint, encoder_config, vc_checkpoint, vc_config, model_dir, voice_dir, use_cuda)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_sample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_tts_from_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_sample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_sample_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/TTS/utils/synthesizer.py\u001b[0m in \u001b[0;36m_load_tts_from_dir\u001b[0;34m(self, model_dir, use_cuda)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_tts_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtts_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/TTS/tts/models/xtts.py\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(self, config, checkpoint_dir, checkpoint_path, vocab_path, eval, strict, use_deepspeed)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_compatible_checkpoint_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;31m# deal with v1 and v1.1. V1 has the init_gpt_for_inference keys, v1.1 do not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/TTS/tts/models/xtts.py\u001b[0m in \u001b[0;36mget_compatible_checkpoint_state_dict\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_compatible_checkpoint_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_fsspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;31m# remove xtts gpt trainer extra keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0mignore_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch_mel_spectrogram_style_encoder\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"torch_mel_spectrogram_dvae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dvae\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/TTS/utils/io.py\u001b[0m in \u001b[0;36mload_fsspec\u001b[0;34m(path, map_location, cache, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1468\u001b[0m                         )\n\u001b[1;32m   1469\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1470\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m                 return _load(\n\u001b[1;32m   1472\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL TTS.config.shared_configs.BaseDatasetConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([BaseDatasetConfig])` or the `torch.serialization.safe_globals([BaseDatasetConfig])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from TTS.api import TTS\n",
        "import fitz  # PyMuPDF\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import torch # Import torch\n",
        "\n",
        "os.makedirs(\"audios\", exist_ok=True)\n",
        "\n",
        "SPEAKER_WAV = \"voz_clonada_sample.wav\"\n",
        "OUTPUT_DIR = \"audios\"\n",
        "\n",
        "# Aceita os termos automaticamente (Coqui)\n",
        "os.environ[\"COQUI_TOS_AGREED\"] = \"1\"\n",
        "\n",
        "# Allowlist XttsConfig and XttsAudioConfig for torch.load\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.config.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.models.xtts import XttsAudioConfig # Import XttsAudioConfig\n",
        "torch.serialization.add_safe_globals([XttsConfig, XttsAudioConfig, BaseDatasetConfig])\n",
        "\n",
        "\n",
        "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", progress_bar=False, gpu=False)\n",
        "\n",
        "emotions = [\"neutral\", \"happy\", \"sad\", \"angry\", \"excited\", \"sleepy\", \"whispering\", \"shouting\"]\n",
        "\n",
        "def extract_text(file):\n",
        "    ext = os.path.splitext(file.name)[-1].lower()\n",
        "    text = \"\"\n",
        "    if ext == \".pdf\":\n",
        "        doc = fitz.open(stream=file.read(), filetype=\"pdf\")\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    elif ext == \".epub\":\n",
        "        book = epub.read_epub(file.name)\n",
        "        for item in book.get_items():\n",
        "            if item.get_type() == epub.EpubHtml:\n",
        "                soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
        "                text += soup.get_text()\n",
        "    return text.strip()\n",
        "\n",
        "def narrar(texto, emocao):\n",
        "    wav_path = os.path.join(OUTPUT_DIR, \"narracao.wav\")\n",
        "    mp3_path = os.path.join(OUTPUT_DIR, \"narracao.mp3\")\n",
        "\n",
        "    tts.tts_to_file(\n",
        "        text=texto,\n",
        "        speaker_wav=SPEAKER_WAV,\n",
        "        language=\"pt\",\n",
        "        emotion=emocao,\n",
        "        file_path=wav_path\n",
        "    )\n",
        "\n",
        "    audio = AudioSegment.from_wav(wav_path)\n",
        "    audio.export(mp3_path, format=\"mp3\")\n",
        "\n",
        "    return wav_path, mp3_path\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Narrador XTTS com sua voz preferida ðŸŽ¤\")\n",
        "\n",
        "    with gr.Row():\n",
        "        arquivo = gr.File(label=\"Carregar PDF ou EPUB\", file_types=[\".pdf\", \".epub\"])\n",
        "        emocao = gr.Dropdown(emotions, label=\"EmoÃ§Ã£o\", value=\"neutral\")\n",
        "\n",
        "    texto_extraido = gr.Textbox(label=\"Texto extraÃ­do\", lines=10)\n",
        "    carregar_btn = gr.Button(\"Extrair texto\")\n",
        "\n",
        "    carregar_btn.click(fn=extract_text, inputs=arquivo, outputs=texto_extraido)\n",
        "\n",
        "    with gr.Row():\n",
        "        narrar_btn = gr.Button(\"Narrar\")\n",
        "        audio_saida = gr.Audio(label=\"Ouvir Ã¡udio\")\n",
        "        download_mp3 = gr.File(label=\"Baixar em MP3\")\n",
        "\n",
        "    narrar_btn.click(fn=narrar, inputs=[texto_extraido, emocao], outputs=[audio_saida, download_mp3])\n",
        "\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}